@article{silver2018general,
  title={A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal={Science},
  volume={362},
  number={6419},
  pages={1140--1144},
  year={2018},
  publisher={American Association for the Advancement of Science}
}
@inproceedings{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  pages={1928--1937},
  year={2016}
}
@article{castro2018dopamine,
  title={Dopamine: A research framework for deep reinforcement learning},
  author={Castro, Pablo Samuel and Moitra, Subhodeep and Gelada, Carles and Kumar, Saurabh and Bellemare, Marc G},
  journal={arXiv preprint arXiv:1812.06110},
  year={2018}
}
@article{howard2020fastai,
  title={Fastai: A layered API for deep learning},
  author={Howard, Jeremy and Gugger, Sylvain},
  journal={Information},
  volume={11},
  number={2},
  pages={108},
  year={2020},
  publisher={Multidisciplinary Digital Publishing Institute}
}
@InProceedings{pmlr-v80-liang18b, title = {{RL}lib: Abstractions for Distributed Reinforcement Learning}, author = {Liang, Eric and Liaw, Richard and Nishihara, Robert and Moritz, Philipp and Fox, Roy and Goldberg, Ken and Gonzalez, Joseph and Jordan, Michael and Stoica, Ion}, pages = {3053--3062}, year = {2018}, editor = {Jennifer Dy and Andreas Krause}, volume = {80}, series = {Proceedings of Machine Learning Research}, address = {Stockholmsm√§ssan, Stockholm Sweden}, month = {10--15 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v80/liang18b/liang18b.pdf}, url = {http://proceedings.mlr.press/v80/liang18b.html}, abstract = {Reinforcement learning (RL) algorithms involve the deep nesting of highly irregular computation patterns, each of which typically exhibits opportunities for distributed computation. We argue for distributing RL components in a composable way by adapting algorithms for top-down hierarchical control, thereby encapsulating parallelism and resource requirements within short-running compute tasks. We demonstrate the benefits of this principle through RLlib: a library that provides scalable software primitives for RL. These primitives enable a broad range of algorithms to be implemented with high performance, scalability, and substantial code reuse. RLlib is available as part of the open source Ray project at http://rllib.io/.} }
@article{stooke2019rlpyt,
  title={rlpyt: A research code base for deep reinforcement learning in pytorch},
  author={Stooke, Adam and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1909.01500},
  year={2019}
}
@inproceedings{engstrom2019implementation,
  title={Implementation Matters in Deep RL: A Case Study on PPO and TRPO},
  author={Engstrom, Logan and Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Janoos, Firdaus and Rudolph, Larry and Madry, Aleksander},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}